{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/sami-ka/gnn_slotting_optimization.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17578a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install polars>=1.36.1\n",
    "# ! pip install pyzmq>=27.1.0 scipy>=1.16.3 torch>=2.9.1 torch-geometric==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886e8fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "root = Path(\"/\")\n",
    "\n",
    "sys.path.insert(0, \"/content/gnn_slotting_optimization\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39073e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slotting_optimization.generator import DataGenerator\n",
    "from slotting_optimization.order_book import OrderBook\n",
    "from slotting_optimization.item_locations import ItemLocations\n",
    "from slotting_optimization.warehouse import Warehouse\n",
    "\n",
    "gen = DataGenerator()\n",
    "samples = gen.generate_samples(20, 20, 300, 1, 10, n_samples=10000, distances_fixed=True, seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d101be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CHANGE 1: Switch to 3D Edge Attributes =====\n",
    "from slotting_optimization.gnn_builder import build_graph_3d_sparse  # CHANGED\n",
    "from slotting_optimization.simulator import Simulator\n",
    "list_data = []\n",
    "for (ob, il, w) in samples:\n",
    "    g_data = build_graph_3d_sparse(  # CHANGED from build_graph_sparse\n",
    "        order_book=ob,\n",
    "        item_locations=il,\n",
    "        warehouse=w,\n",
    "        simulator=Simulator().simulate\n",
    "    )\n",
    "    list_data.append(g_data)\n",
    "\n",
    "print(f\"✓ Generated {len(list_data)} graphs with 3D edge attributes\")\n",
    "print(f\"  First graph: {list_data[0].num_nodes} nodes, {list_data[0].edge_index.shape[1]} edges\")\n",
    "print(f\"  Edge attributes shape: {list_data[0].edge_attr.shape}\")  # Should be [num_edges, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3fe5c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(12345)\n",
    "train_split_idx = int(0.8 * len(list_data))\n",
    "train_dataset = list_data[:train_split_idx]\n",
    "test_dataset = list_data[train_split_idx:]\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cn70p9ubvb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 1: Compute Baseline Performance =====\n",
    "import numpy as np\n",
    "\n",
    "# Extract all target values from datasets\n",
    "train_targets = np.array([data.y.item() for data in train_dataset])\n",
    "test_targets = np.array([data.y.item() for data in test_dataset])\n",
    "all_targets = np.concatenate([train_targets, test_targets])\n",
    "\n",
    "# Compute statistics\n",
    "train_mean = train_targets.mean()\n",
    "train_std = train_targets.std()\n",
    "train_var = train_targets.var()\n",
    "train_min = train_targets.min()\n",
    "train_max = train_targets.max()\n",
    "\n",
    "# Baseline MSE = variance (always predicting mean)\n",
    "baseline_mse = train_var\n",
    "baseline_rmse = np.sqrt(baseline_mse)\n",
    "normalized_rmse = baseline_rmse / train_mean  # RMSE as % of mean\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training set statistics:\")\n",
    "print(f\"  Mean distance:     {train_mean:,.2f}\")\n",
    "print(f\"  Std deviation:     {train_std:,.2f}\")\n",
    "print(f\"  Min distance:      {train_min:,.2f}\")\n",
    "print(f\"  Max distance:      {train_max:,.2f}\")\n",
    "print(f\"\\nBaseline (always predict mean):\")\n",
    "print(f\"  MSE:               {baseline_mse:,.2f}\")\n",
    "print(f\"  RMSE:              {baseline_rmse:,.2f}\")\n",
    "print(f\"  Normalized RMSE:   {normalized_rmse:.2%} of mean\")\n",
    "print(f\"\\nTest set mean:       {test_targets.mean():,.2f}\")\n",
    "print(f\"Test set std:        {test_targets.std():,.2f}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n⚠️  Current model MSE (~1e6) vs baseline MSE ({baseline_mse:.2e})\")\n",
    "print(f\"   Goal: Beat baseline by achieving MSE < {baseline_mse:.2e}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b8eyyd7iw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 2: Normalize Targets =====\n",
    "# Normalize targets to have mean=0, std=1\n",
    "# This makes optimization much easier\n",
    "\n",
    "# Compute normalization parameters from training set ONLY\n",
    "mean_y = train_mean\n",
    "std_y = train_std\n",
    "\n",
    "print(f\"\\nTarget normalization parameters:\")\n",
    "print(f\"  mean_y = {mean_y:,.2f}\")\n",
    "print(f\"  std_y  = {std_y:,.2f}\")\n",
    "\n",
    "# Normalize all targets in both train and test datasets\n",
    "for data in train_dataset:\n",
    "    data.y = (data.y - mean_y) / std_y\n",
    "\n",
    "for data in test_dataset:\n",
    "    data.y = (data.y - mean_y) / std_y\n",
    "\n",
    "# Verify normalization\n",
    "train_y_norm = np.array([data.y.item() for data in train_dataset])\n",
    "test_y_norm = np.array([data.y.item() for data in test_dataset])\n",
    "\n",
    "print(f\"\\nAfter normalization:\")\n",
    "print(f\"  Train: mean={train_y_norm.mean():.4f}, std={train_y_norm.std():.4f}\")\n",
    "print(f\"  Test:  mean={test_y_norm.mean():.4f}, std={test_y_norm.std():.4f}\")\n",
    "print(f\"\\n✓ Targets normalized! Loss scale should now be ~1.0\")\n",
    "print(\"  (Remember to denormalize predictions: y_pred * std_y + mean_y)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gqw77joqcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CHANGE 2: Normalize Edge Attributes =====\n",
    "# Normalize 3D edge attributes to have mean=0, std=1 per dimension\n",
    "\n",
    "# Compute edge attribute statistics from training set\n",
    "all_edge_attrs = torch.cat([data.edge_attr for data in train_dataset], dim=0)\n",
    "edge_mean = all_edge_attrs.mean(dim=0)  # [3]\n",
    "edge_std = all_edge_attrs.std(dim=0)    # [3]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EDGE ATTRIBUTE NORMALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Edge attribute dimensions: [distance, sequence_count, assignment]\")\n",
    "print(f\"  Mean: {edge_mean}\")\n",
    "print(f\"  Std:  {edge_std}\")\n",
    "\n",
    "# Normalize edge attributes in both train and test datasets\n",
    "for data in train_dataset:\n",
    "    data.edge_attr = (data.edge_attr - edge_mean) / (edge_std + 1e-8)\n",
    "\n",
    "for data in test_dataset:\n",
    "    data.edge_attr = (data.edge_attr - edge_mean) / (edge_std + 1e-8)\n",
    "\n",
    "# Verify normalization\n",
    "all_edge_attrs_norm = torch.cat([data.edge_attr for data in train_dataset], dim=0)\n",
    "print(f\"\\nAfter normalization:\")\n",
    "print(f\"  Mean: {all_edge_attrs_norm.mean(dim=0)}\")\n",
    "print(f\"  Std:  {all_edge_attrs_norm.std(dim=0)}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✓ Edge attributes normalized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de605bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e196cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61163967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeThenNodeLayer(MessagePassing):\n",
    "    def __init__(self, node_dim, edge_dim):\n",
    "        super().__init__(aggr=\"add\")\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * node_dim + edge_dim, edge_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(edge_dim, edge_dim),\n",
    "        )\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(node_dim + edge_dim, node_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(node_dim, node_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        row, col = edge_index\n",
    "        edge_attr = self.edge_mlp(\n",
    "            torch.cat([x[row], x[col], edge_attr], dim=1)\n",
    "        )\n",
    "        x = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "        return x, edge_attr\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return self.node_mlp(torch.cat([x_j, edge_attr], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5773ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeThenEdgeLayer(MessagePassing):\n",
    "    def __init__(self, node_dim, edge_dim):\n",
    "        super().__init__(aggr=\"add\")\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(node_dim + edge_dim, node_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(node_dim, node_dim),\n",
    "        )\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * node_dim + edge_dim, edge_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(edge_dim, edge_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "        row, col = edge_index\n",
    "        edge_attr = self.edge_mlp(\n",
    "            torch.cat([x[row], x[col], edge_attr], dim=1)\n",
    "        )\n",
    "        return x, edge_attr\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return self.node_mlp(torch.cat([x_j, edge_attr], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim):\n",
    "        super().__init__()\n",
    "        self.edge_then_node = EdgeThenNodeLayer(node_dim, edge_dim)\n",
    "        self.node_then_edge = NodeThenEdgeLayer(node_dim, edge_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x, edge_attr = self.edge_then_node(x, edge_index, edge_attr)\n",
    "        x, edge_attr = self.node_then_edge(x, edge_index, edge_attr)\n",
    "        return x, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48daa307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CHANGE 3: Fix GraphRegressionModel =====\n",
    "# Remove batch-dependent node embeddings, use small random initialization\n",
    "\n",
    "class GraphRegressionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, edge_dim, num_layers):  # REMOVED num_nodes\n",
    "        super().__init__()\n",
    "\n",
    "        # REMOVED: self.node_embedding = nn.Embedding(num_nodes, hidden_dim)\n",
    "        # Instead, we'll initialize node features with small random values in forward()\n",
    "\n",
    "        self.hidden_dim = hidden_dim  # Store for use in forward()\n",
    "        self.edge_encoder = nn.Linear(edge_dim, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GCNBlock(hidden_dim, hidden_dim) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # CHANGED: Initialize node features with small random values\n",
    "        # Small random initialization helps with gradient flow (better than all zeros)\n",
    "        x = torch.randn((data.num_nodes, self.hidden_dim), \n",
    "                       device=data.edge_index.device) * 0.01\n",
    "\n",
    "        edge_attr = self.edge_encoder(data.edge_attr)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x, edge_attr = layer(x, data.edge_index, edge_attr)\n",
    "\n",
    "        graph_emb = global_add_pool(x, data.batch)\n",
    "        out = self.regressor(graph_emb)\n",
    "        return out.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c29c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pytorch model checkpoint from pt file\n",
    "import torch\n",
    "\n",
    "# Load the checkpoint (it's a dictionary, not the model itself)\n",
    "checkpoint = torch.load(\"final_model.pt\", weights_only=False)\n",
    "\n",
    "# You need to recreate the model architecture first, then load the state dict\n",
    "# From your training notebook, the model was:\n",
    "model = GraphRegressionModel(\n",
    "    hidden_dim=64,\n",
    "    edge_dim=3,\n",
    "    num_layers=5\n",
    ")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# Get normalization parameters for denormalizing predictions\n",
    "mean_y = checkpoint[\"mean_y\"]\n",
    "std_y = checkpoint[\"std_y\"]\n",
    "\n",
    "# Read test set\n",
    "test_data = torch.load(\"test_dataset.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ed38cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CHANGE 4: Update Model Instantiation =====\n",
    "# Updated for 3D edge attributes, removed num_nodes, and increased layers\n",
    "\n",
    "model = GraphRegressionModel(\n",
    "    hidden_dim=64,\n",
    "    edge_dim=3,  # CHANGED from 1 to 3 for 3D edge attributes\n",
    "    num_layers=5  # CHANGED from 3 to 5 for better message passing\n",
    ")\n",
    "print(model)\n",
    "print(f\"\\n✓ Model updated:\")\n",
    "print(f\"  hidden_dim = 64\")\n",
    "print(f\"  edge_dim = 3 (3D edge attributes)\")\n",
    "print(f\"  num_layers = 5 (increased for better information flow)\")\n",
    "print(f\"  Node features: small random initialization (0.01 * randn)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b9c8908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a331062",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "inputs = data.to(device)\n",
    "targets = data.y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f3414d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEPS 4-5: Updated Training with Diagnostics =====\n",
    "import torch\n",
    "\n",
    "# CRITICAL FIX: Reduced learning rate from 0.001 to 0.0001 (prevents explosion)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # CHANGED from 0.001\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Add learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=10,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tracking metrics\n",
    "train_batch_losses = []\n",
    "train_epoch_losses = []\n",
    "val_epoch_losses = []\n",
    "grad_norms = []\n",
    "learning_rates = []\n",
    "\n",
    "\n",
    "def compute_gradient_norm(model):\n",
    "    \"\"\"Compute the total gradient norm across all parameters.\"\"\"\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = data.y.size(0)\n",
    "        train_batch_losses.append(loss.item())\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        n_samples += batch_size\n",
    "\n",
    "    train_epoch_losses.append(epoch_loss / n_samples)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    mse = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            pred = model(data)\n",
    "            mse += ((pred - data.y) ** 2).sum().item()\n",
    "\n",
    "    return mse / len(loader.dataset)\n",
    "\n",
    "\n",
    "def log_diagnostics(epoch, train_mse, val_mse, grad_norm, lr):\n",
    "    \"\"\"Print diagnostic information.\"\"\"\n",
    "    if epoch == 1 or epoch % 10 == 0:\n",
    "        # Denormalize MSE for interpretability\n",
    "        train_mse_denorm = train_mse * (std_y ** 2)\n",
    "        val_mse_denorm = val_mse * (std_y ** 2)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch:03d}:\")\n",
    "        print(f\"  Train MSE (norm):   {train_mse:.6f}\")\n",
    "        print(f\"  Val MSE (norm):     {val_mse:.6f}\")\n",
    "        print(f\"  Train MSE (orig):   {train_mse_denorm:,.2f}\")\n",
    "        print(f\"  Val MSE (orig):     {val_mse_denorm:,.2f}\")\n",
    "        print(f\"  Baseline MSE:       {baseline_mse:,.2f}\")\n",
    "        print(f\"  Improvement:        {(1 - val_mse_denorm/baseline_mse)*100:.2f}%\")\n",
    "        print(f\"  Grad norm:          {grad_norm:.6f}\")\n",
    "        print(f\"  Learning rate:      {lr:.6f}\")\n",
    "        \n",
    "        # Check for issues\n",
    "        if grad_norm < 1e-6:\n",
    "            print(f\"  ⚠️  WARNING: Gradients very small ({grad_norm:.2e})\")\n",
    "        elif grad_norm > 100:\n",
    "            print(f\"  ⚠️  WARNING: Gradients large ({grad_norm:.2e})\")\n",
    "\n",
    "\n",
    "best_val_mse = float(\"inf\")\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING START - ATTEMPT 2\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Learning rate: {optimizer.param_groups[0]['lr']} (LOWERED to prevent explosion)\")\n",
    "print(f\"Gradient clipping: max_norm=1.0\")\n",
    "print(f\"Num layers: 5 (increased for better message passing)\")\n",
    "print(f\"Node init: small random (0.01 * randn)\")\n",
    "print(f\"Target normalization: mean={mean_y:.2f}, std={std_y:.2f}\")\n",
    "print(f\"Baseline MSE to beat: {baseline_mse:,.2f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(1, 201):  # Start with 50 epochs\n",
    "    train()\n",
    "    \n",
    "    # Compute gradient norm after training step\n",
    "    # (Do a dummy forward-backward to get current gradients)\n",
    "    model.train()\n",
    "    sample_data = next(iter(train_loader)).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    sample_out = model(sample_data)\n",
    "    sample_loss = criterion(sample_out, sample_data.y)\n",
    "    sample_loss.backward()\n",
    "    grad_norm = compute_gradient_norm(model)\n",
    "    grad_norms.append(grad_norm)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_mse = test(train_loader)\n",
    "    val_mse = test(test_loader)\n",
    "    val_epoch_losses.append(val_mse)\n",
    "    \n",
    "    # Get current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    # Log diagnostics\n",
    "    log_diagnostics(epoch, train_mse, val_mse, grad_norm, current_lr)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(val_mse)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_mse < best_val_mse:\n",
    "        best_val_mse = val_mse\n",
    "        best_epoch = epoch\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"val_mse\": val_mse,\n",
    "                \"mean_y\": mean_y,\n",
    "                \"std_y\": std_y,\n",
    "            },\n",
    "            \"best_model.pt\"\n",
    "        )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "print(f\"Best val MSE (normalized): {best_val_mse:.6f}\")\n",
    "print(f\"Best val MSE (original):   {best_val_mse * std_y**2:,.2f}\")\n",
    "print(f\"Baseline MSE:              {baseline_mse:,.2f}\")\n",
    "print(f\"Improvement over baseline: {(1 - (best_val_mse * std_y**2)/baseline_mse)*100:.2f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "vz0u9i3sd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 6: Enhanced Visualizations =====\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Training and Validation Loss\n",
    "ax1 = axes[0, 0]\n",
    "epochs_range = range(1, len(train_epoch_losses) + 1)\n",
    "ax1.plot(epochs_range, train_epoch_losses, label='Train Loss', marker='o', alpha=0.7)\n",
    "ax1.plot(epochs_range, val_epoch_losses, label='Val Loss', marker='s', alpha=0.7)\n",
    "ax1.axhline(y=1.0, color='r', linestyle='--', label='Normalized Baseline (variance=1.0)', alpha=0.5)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE (Normalized)')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Batch Loss (smoothed)\n",
    "ax2 = axes[0, 1]\n",
    "def moving_average(values, window):\n",
    "    return [\n",
    "        sum(values[max(0, i - window):i + 1]) /\n",
    "        (i - max(0, i - window) + 1)\n",
    "        for i in range(len(values))\n",
    "    ]\n",
    "smoothed_batch_losses = moving_average(train_batch_losses, window=50)\n",
    "ax2.plot(train_batch_losses, alpha=0.2, label='Batch Loss', color='blue')\n",
    "ax2.plot(smoothed_batch_losses, label='Smoothed Batch Loss', color='orange', linewidth=2)\n",
    "ax2.set_xlabel('Training Step')\n",
    "ax2.set_ylabel('MSE (Normalized)')\n",
    "ax2.set_title('Batch-Level Training Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Gradient Norms\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(epochs_range, grad_norms, marker='o', color='green')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Gradient Norm')\n",
    "ax3.set_title('Gradient Norm Over Time')\n",
    "ax3.axhline(y=1.0, color='r', linestyle='--', alpha=0.3, label='Target norm (~1.0)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "# 4. Learning Rate Schedule\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(epochs_range, learning_rates, marker='o', color='purple')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Learning Rate')\n",
    "ax4.set_title('Learning Rate Schedule')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_diagnostics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nLoss progression:\")\n",
    "print(f\"  Initial train loss: {train_epoch_losses[0]:.6f}\")\n",
    "print(f\"  Final train loss:   {train_epoch_losses[-1]:.6f}\")\n",
    "print(f\"  Initial val loss:   {val_epoch_losses[0]:.6f}\")\n",
    "print(f\"  Final val loss:     {val_epoch_losses[-1]:.6f}\")\n",
    "print(f\"\\nLoss reduction:\")\n",
    "print(f\"  Train: {(1 - train_epoch_losses[-1]/train_epoch_losses[0])*100:.2f}%\")\n",
    "print(f\"  Val:   {(1 - val_epoch_losses[-1]/val_epoch_losses[0])*100:.2f}%\")\n",
    "print(f\"\\nGradient norms:\")\n",
    "print(f\"  Mean: {np.mean(grad_norms):.6f}\")\n",
    "print(f\"  Min:  {np.min(grad_norms):.6f}\")\n",
    "print(f\"  Max:  {np.max(grad_norms):.6f}\")\n",
    "print(f\"\\nLearning rate:\")\n",
    "print(f\"  Initial: {learning_rates[0]:.6f}\")\n",
    "print(f\"  Final:   {learning_rates[-1]:.6f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6d5f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_batch_losses = moving_average(train_batch_losses[50:], window=50)\n",
    "plt.figure()\n",
    "plt.plot(train_batch_losses[50:], alpha=0.3, label=\"Batch loss\")\n",
    "plt.plot(smoothed_batch_losses, label=\"Smoothed batch loss\")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_epoch_losses[50:], marker=\"o\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Train MSE\")\n",
    "plt.title(\"Epoch level training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b15021be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"val_mse\": val_mse,\n",
    "                \"mean_y\": mean_y,\n",
    "                \"std_y\": std_y,\n",
    "            },\n",
    "            \"final_model.pt\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e0c2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b429549",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27e8b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e471211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /content/gnn_slotting_optimization /content/drive/MyDrive/\n",
    "!cp final_model.pt best_model.pt training_diagnostics.png /content/drive/MyDrive/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33ee4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-slotting-optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
